% 1D and 2D DNG on C++ and GPU
\chapter{GPU Implementation of 1D and 2D DNG Slab}
\section{GPU Programming Model\index{GPU!programming model}}
\subsection{Evolution of GPUs\index{GPU!evolution}}
With the evolution of 3D graphics arose a need for faster computing means to handle real--time graphics processing. The graphics processing unit or GPU\index{GPU} was originally meant to act as a separate processor to handle graphics computations. The process of transforming a 3D scenario to a 2D image that can be displayed on a computer screen is known as \emph{rendering}\index{render}. Rendering involves determining the exact colour or shade of each pixel and geometry calculations. In the earliest GPUs these were referred to as pixel\index{shader!pixel} and vertex\index{shader!vertex} shading. The GPUs had discrete processing units known as pixel and vertex shaders that performed these calculations. As an example, the nVidia's\index{nVidia} GeForce 6200 GPU had four pixel shaders and three vertex shaders \cite{Ref:Geforce6-wiki}. Table \ref{Tab:Eary-GPU-Comparison} provides a comparison of some typical GPUs. The number of pixel shaders are significantly greater than vertex shaders due to unequal workload.
\begin{table}[H]
\begin{center}
\vspace{0.3cm}
	\begin{tabular}{lccc}
	\hline \hline
		\rule{0pt}{2.6ex} & \textbf{GeForce 6200} & \textbf{GeForce 6600} & \textbf{ATi x850}\\
		\hline
		Transistor Count \rule{0pt}{2.6ex} & 77 million & 222 million & 160 million\\
		Process & 0.11 $\mu$m & 0.11 $\mu$m & 0.13 $\mu$m low--k\\
		Pixel Shaders & 4 & 8 & 16\\
		Vertex Shaders & 3 & 3 & 6\\
	\hline \hline
	\end{tabular}
\end{center}
\caption{Comparison of some early GPUs}
\label{Tab:Eary-GPU-Comparison}
\end{table}
\subsection{Unified Shader Architecture\index{unified shader!architecture}}
GPUs\index{GPU} were already far ahead of contemporary CPUs\index{CPU} in terms of computation power when the first line of these new GPUs based on unified shader model\index{unified shader!architecture} arrived in 2005 \cite{Ref:Unified-Shader-Architecture-devmaster.net}. In the unified shader model\index{unified shader!model} shading units are not confined to just pixel or vertex calculations. They can operate on any \emph{shader instruction}\index{shader!instruction} \cite{Ref:Unified-Shader-Model-wiki}. Each shader is capable of doing same set of defined arithmetic calculations. It was possible to use GPU as a general-purpose computing device\index{GPU!general purpose (GPGPU)}.

Major GPU manufacturers, nVidia\index{nVidia} and AMD/ATi\index{AMD/ATi} provide application programming interface (API)\index{API} and software development kits (SDKs)\index{SDK} to program their GPUs. CUDA\index{CUDA} or compute unified device architecture is the API provided by nVidia to program their GPUs. Additionally, an open standard OpenCL\index{OpenCL} has evolved for GPU computing that has been adopted by both nVidia and AMD/ATi. AMD/ATi API is based on OpenCL standard.

To take advantage of GPU acceleration a problem must have either task--parallel or data--parallel nature.
\subsection{Task--Parallelism\index{parallelism!task}}
In task--parallelism computation consist of several independent tasks that run concurrently. These tasks may be completely unrelated but the end result is dependent on their outputs. Consider summation of a series, where we want to compute the value of $e^x$ from Taylor series. The mathematical expression is given by
\begin{equation}
e^x = 1+\dfrac{x^1}{1!}+\dfrac{x^2}{2!}+\dfrac{x^3}{3!}+\dfrac{x^4}{4!}+\dfrac{x^5}{5!}+\dfrac{x^6}{6!}+...+\dfrac{x^n}{n!}
\label{eq:ex-Taylor-Series}
\end{equation}

A single--threaded\index{threading!single} conventional implementation would have to calculate all the terms one--by--one and then sum up the result in the end. However, in a multi--threaded\index{threading!multi} implementation, each thread would calculate only one term. All the treads will perform their computation in parallel and the end result is then summed up. The computation--intensive task of calculating factorials and higher powers of $x$ is parallelised and results in significant reduction of computation time.
\subsection{Data--Parallelism\index{parallelism!data}}
In data--parallelism same operation is performed on individual elements of data. A simple example is that of scalar matrix multiplication. Consider an array of size $n$ being multiplied with a scalar constant $c$. The result of each multiplication can be calculated independently by assigning a separate thread for the task. This is illustrated in figure \ref{fig:Data-Parallelism} where input array is \texttt{A[]} and resultant array is \texttt{B[]}. Data--parallelism applies to any scenario where values in resultant data array only depends on values from input array. FDTD\index{FDTD!data--parallelism} is a good example of data--parallelism and a GPU implementation can take advantage of accelerated computing.
\begin{figure}[H]
\centering
\subfigure[Single--threaded implementation]{
\begin{tikzpicture}
	\newcommand{\Xdisp}{0cm}
	\newcommand{\Ydisp}{0cm}
	% e^x;
	\coordinate [label=center:$e^x\rightarrow$] (exequals) at (\Xdisp-0.75cm,\Ydisp+0.75cm);
	% Single-threaded flow chart.
	\draw (\Xdisp-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^0}{0!}$] (Term0ST) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	\renewcommand{\Xdisp}{1.25cm}
	\renewcommand{\Ydisp}{-1.25cm}
	\draw (\Xdisp-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^1}{1!}$] (Term1ST) at (\Xdisp+0.5cm,\Ydisp-0.5cm);
	\filldraw[fill=white] (\Xdisp+0.5cm,\Ydisp+0.75cm) circle (0.25cm);
	\coordinate [label=center:$+$] (plusTerm1ST) at (\Xdisp+0.5cm,\Ydisp+0.75cm);

	\renewcommand{\Xdisp}{2.5cm}
	\renewcommand{\Ydisp}{-2.5cm}
	\draw (\Xdisp-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^2}{2!}$] (Term2ST) at (\Xdisp+0.5cm,\Ydisp-0.5cm);
	\filldraw[fill=white] (\Xdisp+0.5cm,\Ydisp+0.75cm) circle (0.25cm);
	\coordinate [label=center:$+$] (plusTerm2ST) at (\Xdisp+0.5cm,\Ydisp+0.75cm);

	\renewcommand{\Xdisp}{3.75cm}
	\renewcommand{\Ydisp}{-3.75cm}
	\draw (\Xdisp-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^3}{3!}$] (Term3ST) at (\Xdisp+0.5cm,\Ydisp-0.5cm);
	\filldraw[fill=white] (\Xdisp+0.5cm,\Ydisp+0.75cm) circle (0.25cm);
	\coordinate [label=center:$+$] (plusTerm3ST) at (\Xdisp+0.5cm,\Ydisp+0.75cm);

	\renewcommand{\Xdisp}{5cm}
	\renewcommand{\Ydisp}{-5cm}
	\draw[dashed] (\Xdisp-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^n}{n!}$] (TermnST) at (\Xdisp+0.5cm,\Ydisp-0.5cm);
	\filldraw[fill=white] (\Xdisp+0.5cm,\Ydisp+0.75cm) circle (0.25cm);
	\coordinate [label=center:$+$] (plusTermnST) at (\Xdisp+0.5cm,\Ydisp+0.75cm);
	% Result
	\coordinate [label=right:$\rightarrow$\textsf{Result}] (ResultST) at (\Xdisp+1.0cm,\Ydisp-0.5cm);
	% Thread.
	\draw[thick, rounded corners, ->, >=stealth] (-0.75cm,-0.5cm) -- (-0.9cm,-0.75cm) -- (-0.6cm,-1.25cm) -- (-0.9cm,-1.75cm) -- (-0.6cm,-2.25cm) -- (-0.9cm,-2.75cm) -- (-0.6cm,-3.25cm) -- (-0.9cm,-3.75cm) -- (-0.6cm,-4.25cm) -- (-0.9cm,-4.75cm) -- (-0.75cm,-5.0cm) -- (-0.75cm,-5.5cm);
\end{tikzpicture}}
\subfigure[Multi--threaded implementation]{
\begin{tikzpicture}
	\newcommand{\Xdisp}{0cm}
	\newcommand{\Ydisp}{0cm}
	% e^x;
	\coordinate [label=center:$e^x\rightarrow$] (exequals) at (\Xdisp-0.75cm,\Ydisp+0.75cm);
	% Single-threaded flow chart.
	\draw (-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^0}{0!}$] (Term0MT) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	\renewcommand{\Xdisp}{2cm}
	\draw (-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^1}{1!}$] (Term1MT) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	\renewcommand{\Xdisp}{4cm}
	\draw (-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^2}{2!}$] (Term2MT) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	\renewcommand{\Xdisp}{6cm}
	\draw (-0.25cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^3}{3!}$] (Term3MT) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	% Dots.
	\coordinate [label=center:\textsf{{\Large ...}}] (Dots1) at (8cm,\Ydisp-0.5cm);
	\coordinate [label=center:\textsf{{\Large ...}}] (Dots2) at (8cm,\Ydisp-3cm);

	\renewcommand{\Xdisp}{9cm}
	\draw (-0.25cm,\Ydisp+0.75cm) -- (\Xdisp-1.5cm,\Ydisp+0.75cm);
	\draw[dashed] (\Xdisp-1.5cm,\Ydisp+0.75cm) -- (\Xdisp-0.5cm,\Ydisp+0.75cm);
	\draw (\Xdisp-0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp+0.75cm) -- (\Xdisp+0.5cm,\Ydisp);
	\draw (\Xdisp,\Ydisp) rectangle (\Xdisp+1.0cm,\Ydisp-1.0cm);
	\coordinate [label=center:$\dfrac{x^n}{n!}$] (TermnMT) at (\Xdisp+0.5cm,\Ydisp-0.5cm);

	% Thread numbers.
	\foreach \x/\t in {0.5cm/0,2.5cm/1,4.5cm/2,6.5cm/3,9.5cm/n}
		\coordinate [label=below:Thread $\t$] (Thread\t) at (\x,-1.1cm);
	% Threads.
	\foreach \x in {0.5cm,2.5cm,4.5cm,6.5cm,9.5cm}
		\draw[thick, rounded corners, ->, >=stealth] (\x+0cm,-1.75cm) -- (\x+0.15cm,-2cm) -- (\x-0.15cm,-2.5cm) -- (\x+0.15cm,-3cm) -- (\x-0.15cm,-3.5cm) -- (\x+0.15cm,-4cm) -- (\x+0cm,-4.25cm) -- (\x+0cm,-4.75cm);
	% Summation lines.
	\foreach \x in {0.5cm,2.5cm,4.5cm,6.5cm,9.5cm}
		\draw (\x,-5cm) -- (4.5cm,-6.5cm);
	% Result.
	\draw[->, >=stealth] (4.5cm,-6.5cm) -- (4.5cm,-7.5cm);
	\coordinate [label=below:\textsf{Result}] (ResultMT) at (4.5cm,-7.5cm);
	% Summation.
	\filldraw[fill=white] (4.5cm,-6.5cm) circle (0.25cm);
	\coordinate [label=center:$+$] (PlusSign) at (4.5cm,-6.5cm);
\end{tikzpicture}}
\caption{Task--parallelism}
\label{fig:Task-Parallelism}
\end{figure}
\begin{figure}[H]
\centering
\begin{tikzpicture}
	\newcommand{\Xdisp}{0cm}
	\newcommand{\Ydisp}{0cm}
	% Array A.
	\coordinate [label=left:\texttt{A[]}] (ArrayA) at (\Xdisp,\Ydisp+0.4cm);
	\foreach \x in {0cm,1cm,2cm,3cm,4cm,5cm,6cm,9cm}
		\draw (\x, \Ydisp) rectangle (\x+1cm,\Ydisp+0.8cm);
	\foreach \x/\t in {0.5cm/A[0],1.5cm/A[1],2.5cm/A[2],3.5cm/A[3],4.5cm/A[4],5.5cm/A[5],6.5cm/A[6],9.5cm/A[n]}
		\coordinate [label=center:\texttt{\t}] (Thread\t) at (\x,\Ydisp+0.4cm);
	% Dots.
	\coordinate [label=center:\textsf{{\Large ...}}] (Dots1) at (8cm,\Ydisp+0.4cm);
	% Threads.
	\renewcommand{\Ydisp}{-0.4cm}
	\foreach \x in {0.5cm,1.5cm,2.5cm,3.5cm,4.5cm,5.5cm,6.5cm,9.5cm}
		\draw[thick, rounded corners, ->, >=stealth] (\x+0cm,\Ydisp+0cm) -- (\x+0.15cm,\Ydisp-0.25cm) -- (\x-0.15cm,\Ydisp-0.75cm) -- (\x+0.15cm,\Ydisp-1.25cm) -- (\x-0.15cm,\Ydisp-1.75cm) -- (\x+0.15cm,\Ydisp-2.25cm) -- (\x+0cm,\Ydisp-2.5cm) -- (\x+0cm,\Ydisp-3cm);
	% Dots.
	\coordinate [label=center:\textsf{{\Large ...}}] (Dots2) at (8cm,\Ydisp-1.4cm);
	% Array B.
	\renewcommand{\Ydisp}{-4.5cm}
	\coordinate [label=left:\texttt{B[]=c*A[]}] (ArrayB) at (\Xdisp,\Ydisp+0.4cm);
	\foreach \x in {0cm,1cm,2cm,3cm,4cm,5cm,6cm,9cm}
		\draw (\x, \Ydisp) rectangle (\x+1cm,\Ydisp+0.8cm);
	\foreach \x/\t in {0.5cm/cA[0],1.5cm/cA[1],2.5cm/cA[2],3.5cm/cA[3],4.5cm/cA[4],5.5cm/cA[5],6.5cm/cA[6],9.5cm/cA[n]}
		\coordinate [label=center:\texttt{\t}] (Thread\t) at (\x,\Ydisp+0.4cm);
	% Dots.
	\coordinate [label=center:\textsf{{\Large ...}}] (Dots3) at (8cm,\Ydisp+0.4cm);
\end{tikzpicture}
\caption{Data--parallelism}
\label{fig:Data-Parallelism}
\end{figure}
\subsection{GPU Programming\index{GPU!programming}}
In the GPU programming terminology CPU is referred to as \emph{host}\index{host} while GPU is called \emph{device}\index{device}. The GPU\index{GPU!memory} has its own memory separate from that of CPU's and can only work on data stored in its own memory. Any data that is to be operated on must first be copied from CPU memory into GPU memory. A special function called \emph{kernel}\index{kernel} is invoked from CPU that is run on GPU and performs its calculations. Afterwards any required result or data is copied back to CPU memory for analysis. The whole process can be summarised as:
\begin{enumerate}
	\item Allocate space for input data in CPU memory.
	\item Initialise input data.
	\item Allocate space for input data in GPU memory.
	\item Copy input data to GPU memory.
	\item Invoke kernel that runs on GPU and performs computations on input data.
	\item Copy output data back from GPU to CPU memory.
	\item Post--processing\index{post--processing} and analysis of output data on CPU.
\end{enumerate}
\section{GPU/C++ Implementation\index{C++}}
\subsection{Host (CPU) and Device (GPU) Code}
Code meant to run on CPU and GPU are compiled by different compilers; a C++ compiler and a specialised compiler provided by the GPU manufacturer to compile kernel code. The kernel code is essentially written in C with limited object--oriented capabilities and C extensions meant for interfacing with the GPU. and provide CUDA and OpenCL provide C++ interface for programming the GPU. The kernel code is stored in a separate file with .cu or .cl extension indicating the programming API (CUDA or OpenCL).

nVidia's CUDA compiler is capable of handling most C++ directives including classes and templates and can act as a standalone compiler when writing GPU applications. In case of OpenCL, any C++ compiler can be used where the GPU compiler is called from within as an OpenCL function call.
\subsection{Object--Oriented Design and Class Methods}
An object--oriented approach\index{object--oriented} requires dividing the problem into modules and sub--tasks that can be implemented as methods. Class definitions of 1D DNG implementation in CUDA and 2D DNG in OpenCL are given in appendix \ref{App:1D-DNG-Class-Definition-CUDA} and \ref{App:2D-DNG-Class-Definition-OpenCL}, respectively. The different class methods are explained in this sections.
\subsubsection{Array Allocation and Indexing}
Space for 2D or 3D array can be allocated as a 1D array where amount of space allocated is equal to product of dimensions of array. Position of an element in 1D array is a function of dimensions and indices. For example, space for a 3D array with dimensions \texttt{I}, \texttt{J} and \texttt{K} can be allocated as
\lstset{language=[ISO]C++, commentstyle=\color{green!50!black}, keywordstyle=\color{blue}, stringstyle=\color{red!60!black}}
\label{lst:3D-Array-Allocation}\index{array!allocation}
\begin{lstlisting}[caption={3D array allocation}]
int *Array3D;
Array3D = new int[I*J*K];
\end{lstlisting}
Index \texttt{(i,j,k)} can be accessed as
\label{lst:3D-Array-Access}\index{array!access}
\begin{lstlisting}[caption={Accessing elements of 3D array}]
int Value = Array3D[i+(j*I)+(k*I*J)];
\end{lstlisting}
\subsubsection{Using Macros for Array Indexing\index{macro}}
Macros make it easier to write equations and also increases readability of code.
\label{lst:Array-Access-Macro}
\begin{lstlisting}[caption={Macro to access array elements}]
#define Hy(i,j,n) Hy_[(i)+IHy*(j)+IHy*JHy*(n)]
#define am0(i,j) am0_[(i)+IHx*(j)]
#define PsiEzY(i,j) PsiEzY_[(i)+IEz*(j)]
\end{lstlisting}
\subsubsection{Constant Member Initialisation}
The constructor\index{constructor} initialises constant data members. The constructor can take some simulation parameters when the object is created. These parameters are optional and will be initialised with default values if the input fields are left empty. Some of the  basic parameters are simulation size, choice of source and interval between saving field data. The field data saved to hard disk is referred to as a snapshot\index{field snapshot}.
\subsubsection{\texttt{AllocateMemoryCPU()}}
On host, electric and magnetic fields are dynamically allocated according to size of the simulation. In addition, permittivity, permeability, Drude and PML parameters are defined for each node. These are pre--initialised before simulation to save computation time.
\subsubsection{\texttt{InitialiseCPU()}}
Field arrays, permittivity, permeability, Drude and PML parameter arrays are initialised on host.
\subsubsection{\texttt{InitialiseCL()}}
This is only for OpenCL. It first searches for a suitable device which can be CPU or GPU. OpenCL provides CPU emulation in case a GPU is not present. This is useful for debugging purposes.
\label{lst:OpenCL-Device-Search}
\begin{lstlisting}[caption={Device selection in OpenCL}]
devices = new cl_device_id[deviceListSize/sizeof(cl_device_id)];
SafeCall(!devices, "Error: No devices found.");
SafeCall(clGetContextInfo(context, CL_CONTEXT_DEVICES, deviceListSize, devices, NULL), "Error: Getting Context Info (device list, clGetContextInfo)");
\end{lstlisting}
\subsubsection{\texttt{AllocateMemoryGPU()} and \texttt{CopyDataCPUtoGPU()}}
Space is allocated for data arrays on the GPU device and data arrays initialised on host are copied onto GPU. GPU should have enough memory to store the data arrays otherwise an error will be returned. In case of CUDA this is done explicitly with \texttt{cudaMalloc()} followed by \texttt{cudaMemcpy()}. On OpenCL special data exchange buffers of type \verb|cl_mem| are created that are associated with host pointers. Data transfer from host to device is implicit in this case.
\label{lst:CUDA-Host-Memory-Allocation}
\begin{lstlisting}[caption={Device memory allocation in CUDA}]
checkCudaErrors(cudaMalloc((void **)&d_Ex_, sizeof(PRECISION)*Size*3));
checkCudaErrors(cudaMalloc((void **)&d_Hy_, sizeof(PRECISION)*Size*3));
\end{lstlisting}
\label{lst:OpenCL-Device-Memory-Buffers}
\begin{lstlisting}[caption={Device memory allocation in OpenCL}]
d_Ez_ = clCreateBuffer(context, CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR, sizeof(PRECISION)*IEz*JEz*3, Ez_, &status);
SafeCall(status, "Error: clCreateBuffer() cannot creae input buffer for Ex_");
d_ee_ = clCreateBuffer(context, CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR, sizeof(PRECISION)*IHx*JHx, ee_, &status);
SafeCall(status, "Error: clCreateBuffer() cannot creae input buffer for ee");
\end{lstlisting}
\subsubsection{\texttt{InitialiseCLKernelsGPU()}\index{kernel!OpenCL!initialisation}}
OpenCL implementation needs to associate all the kernels defined in the .cl file with their respective handles. These handles are variables that are used to call kernel from host. The .cl code file is loaded and compiled\index{kernel!OpenCL!compilation} first. Errors and warnings in kernel code are output on console if kernel compiler is unsuccessful in building the program. Kernel arguments\index{kernel!OpenCL!arguments} must be specified before a kernel launch. This will bind the OpenCL memory buffers for data exchange between host and device.
\label{lst:OpenCL-Setting-Kernel-Arguments}
\begin{lstlisting}[caption={Setting kernel arguments in OpenCL}]
SafeCall(clSetKernelArg(DryRun_kernel_M, 30, sizeof(cl_mem), (void *)&d_By_), "Error: Setting kernel argument d_By_");
SafeCall(clSetKernelArg(DryRun_kernel_M, 31, sizeof(cl_mem), (void *)&d_PsiEzX_), "Error: Setting kernel argument d_PsiEzX_");
\end{lstlisting}
\subsubsection{\texttt{DryRun()}\index{dry run} and \texttt{Simulation()}}
A dry run is meant to record incident fields in the absence of an obstacle or scatterer\index{scatterer}. Transmitted fields in the presence of scatterer are recorded in actual simulation. Incident and transmitted fields are used to calculate the transmission\index{coefficient!transmission ($\tau$)} and reflection\index{coefficient!reflection ($\Gamma$)} coefficients. The simulation function call takes an optional boolean argument to specify if field snapshots should be saved to hard disk.
\subsubsection{\texttt{CompleteRun()}}
A complete run method for CPU or GPU is meant to encapsulate all the underlying function calls and provide a higher level abstraction. This takes care of memory allocation, initialisation and clean--up.
\subsection{Device Kernels\index{kernel!call}}
It is necessary to synchronise all the threads calculating magnetic field before electric field calculation. Therefore, for each time step there are two kernel calls, one for magnetic and one for electric field computation. The ABC for 1D simulation requires threads at end--points to be synchronised. This is achieved in CUDA kernel with a \verb|__synchrnoizeThreads()|\index{thread synchronisation!CUDA} directive while in OpenCL the equivalent function is \verb|barrier()|\index{thread synchronisation!OpenCL}.

In CUDA, kernels are invoked\index{kernel!invocation} with a special directive wrapped in \verb|<<< >>>| while in OpenCL kernels are called by their respective handles. Examples of CUDA and OpenCL kernel invocations are given below. CUDA and OpenCL kernel code of 1D and 2D DNG simulations is given in appendices \ref{App:CUDA-1D-DNG-Kernels} and \ref{App:OpenCL-2D-DNG-Kernels}, respectively.
\label{lst:CUDA-Kernel-Call}
\begin{lstlisting}[caption={CUDA Kernel Call}]
FDTD1DDNGKernel_Simulation_E <ThreadsX, ThreadsY> <<<Blocks, Threads>>>(
					Size,
					PulseWidth,
					td,
					SourceLocation,
					SourceChoice,
					e0,
					u0,
					dt,
					dz,
					Sc,
					f,
					fp,
					dr,
					d_Ex_, d_Dx_, d_Hy_, d_By_,
					d_einf, d_uinf, d_wpesq, d_wpmsq, d_ge, d_gm,
					d_ae0, d_ae, d_be, d_ce, d_de, d_ee,
					d_am0, d_am, d_bm, d_cm, d_dm, d_em,
					d_Ext, d_Extt, d_Exz1, d_Exz2,
					x1, Z1, Z2,
					n,
					np,
					n0,
					nf);
\end{lstlisting}
\label{lst:OpenCL-Kernel-Call}
\begin{lstlisting}[caption={OpenCL Kernel Call}]
status = clEnqueueNDRangeKernel(
					commandQueue,
					Simulation_kernel_M,
					2,
					NULL,
					globalThreads,
					localThreads,
					0,
					NULL,
					&events[0]);
\end{lstlisting}
\section{Results}

\section{Performance Analysis}
\subsection{Hardware and Software Set--up}
Matlab, C++, CUDA and OpenCL implementations are tested with respect to space, time and space--time. Operating system, platform/configuration and compiler/toolchain for these implementations are listed in table \ref{Tab:OS-Platform/Configuration-Compiler/Toolchain-for-Testing}. Simulations use \verb|double| data type for arrays on 64 bit optimised platform. The CPU is an Intel Core 2 Duo E8400 @3.00 GHz with 4 GB RAM. For CUDA simulations, GPU is GTX 500 Ti with 192 shaders and 1 GB of memory. Visual C++ 2010 Express is the IDE used on 64 bit Windows 7. The flavour of linux is Fedora 14 64 bit. Hardware and software configuration is listed in table \ref{Tab:Hardware-Software-Configuration-for-Testing}.
\begin{table}[H]
\begin{center}
\vspace{0.3cm}
	\begin{tabular}{cccc}
	\hline \hline
		\rule{0pt}{2.6ex} \multirow{2}{*}{\textbf{API}} & \multirow{2}{*}{\textbf{OS}} & \textbf{Platform/} & \textbf{Compiler/}\\
		& & \textbf{Configuration} & \textbf{Toolchain}\\
		\hline
		Matlab \rule{0pt}{2.6ex} & win, linux & x64 & NA\\
		C++ & win, cygwin, linux & x64/O3 & VC++, gcc/g++\\
		OpenCL & win, linux & x64/O3 & VC++, gcc/g++\\
		CUDA & win, linux & x64/O3 & nvcc for win/linux\\
	\hline \hline
	\end{tabular}
\end{center}
\caption{Operating system, platform/configuration and compiler/toolchain used for performance testing}
\label{Tab:OS-Platform/Configuration-Compiler/Toolchain-for-Testing}
\end{table}
\begin{table}[H]
\begin{center}
\vspace{0.3cm}
	\begin{tabular}{ll}
	\hline \hline
		\textbf{CPU} \rule{0pt}{2.6ex}& Intel Core 2 Duo E8400 @3.00 GHz\\
		\textbf{RAM} & 4.00 GB DDR2\\
		\textbf{GPU} & nVidia Geforce GTX 550 Ti 1 GB\\
		\textbf{Matlab} & 2010a 64 bit\\
		\textbf{Linux} & Fedora 14 64 bit\\
		\textbf{Windows} & Win7 64 bit\\
		\textbf{Cygwin} & 64 bit on Win7\\
	\hline \hline
	\end{tabular}
\end{center}
\caption{Hardware and software used for performance testing}
\label{Tab:Hardware-Software-Configuration-for-Testing}
\end{table}
\subsection{Spatial Performance Analysis}

\begin{figure}[H]
\centering
\begin{tikzpicture}
	\DrawAxes{array~size}{time~(sec)}
	\GridOn
	\TicksOn
	\XAxisTextEven{}{2^2}{4}{6}{8}{10}
	\YAxisTextEven{}{2}{4}{6}{8}{10}
	\draw[red] (0cm,0cm) -- (1cm,0.5cm) -- (2cm,1.1cm) -- (3cm,1.4cm) -- (4cm,1.7cm) -- (5cm,2.6cm) -- (6cm,3.3cm) -- (7cm,5.5cm) -- (8cm,4cm) -- (9cm,4.8cm);
	\DrawLegend
\end{tikzpicture}
\caption{Performance of 1D simulations with respect to space}
\label{fig:Performance-1D-space}
\end{figure}

\begin{table}[H]
\begin{center}
\vspace{0.3cm}
	\begin{tabular}{ccccc}
	\hline \hline
		\rule{0pt}{2.6ex} \multirow{2}{*}{\textbf{API}} & \multirow{2}{*}{\textbf{OS}} & \multirow{2}{*}{\textbf{CPU/GPU}} & \multirow{2}{*}{\textbf{Size}} & \textbf{Time}\\
		& &  & & \textbf{taken (s)}\\
		\hline
		Matlab \rule{0pt}{2.6ex} & win & CPU & &\\
		\hline
		\multirow{2}{*}{gcc/g++} \rule{0pt}{2.6ex} & cygwin & CPU & &\\
		& linux & CPU & &\\
		\hline
		\multirow{4}{*}{OpenCL} \rule{0pt}{2.6ex} & linux & CPU (emu) & &\\
		& win & CPU (emu) & &\\
		& linux & GPU & &\\
		& win & GPU & &\\
		\hline
		\multirow{2}{*}{CUDA} \rule{0pt}{2.6ex} & linux & GPU & &\\
		& win & GPU & &\\
	\hline \hline
	\end{tabular}
\end{center}
\caption{Spatial performance of 1D simulations for 1024 time steps}
\label{Tab:Performance-1D-space}
\end{table}

